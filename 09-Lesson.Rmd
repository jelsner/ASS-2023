# Tuesday October 4, 2022 {.unnumbered}

**"You haven't mastered a tool until you understand when it should not be used."** -- Kelsey Hightower

A former PhD student from this department, who I had the pleasure to supervise, is looking for a masters-level geographer to assist with disease mapping and basic spatial analysis as applied to livestock disease.

To support non-spatial scientists (virologists, field epidemiologists) and post-docs (some with spatial analysis expertise) in the mapping and visualizing of disease occurrence data from across the US and encouraged to develop independent projects leading to publication. Flexible work location especially if working toward a PhD.

Here's the link: <https://www.zintellect.com/Opportunity/Details/USDA-ARS-2022-0343>

My answers to Lab 2 are on Canvas.

Making maps with {rdeck} <https://www.mrworthington.com/articles/rstats/mapping-in-r/> Excellent resource on data visualization <https://clauswilke.com/dataviz/>

Today

-   Defining spatial neighborhoods and spatial weights
-   Computing spatial autocorrelation
-   Spatial lag and its relation to autocorrelation
-   Other neighbor definitions

This lesson marks a departure. You will continue to learn how to code in R, but you will do it in the context of statistical thinking, analysis, and modeling. I find statistics to be a natural extension to how I think about the world, but I realize this comes with experience and may not be natural to you.

## Defining spatial neighborhoods and spatial weights {.unnumbered}

Autocorrelation plays a central role in spatial statistics. It quantifies the degree to which things nearby tend to cluster.

Things include values of variables (attributes) aggregated to polygons (or raster cells) as well as the spatial locations of objects. How autocorrelation gets estimated depends on the geometry of the spatial data.

Things tend to cluster because of:

-   Association: whatever causes an attribute to have a certain value in one area causes the same attribute to have a similar value in areas nearby. Crime rates in nearby neighborhoods might tend to cluster due to similar factors.

-   Causality: something within a given area directly influences outcomes within nearby areas. Non-infectious diseases (e.g., lung cancer) have similar rates in neighborhoods close to an oil refinery.

-   Interaction: the movement of people, goods or information creates relationships between areas. COVID spreads through areas through the movement of people.

Spatial statistics focuses on quantifying, and conditioning on, autocorrelation but it is silent about physical causes. Understanding the reason for autocorrelation in your data is important for inference because the causal mechanism might be confounded by its presence. Divorce rates tend to cluster high in southern states, but so does the number of Waffle Houses. Understanding causation requires domain specific knowledge.

When a variable's values are aggregated (summed or averaged) to regions, autocorrelation is quantified by calculating how similar a value in region $i$ is to the value in region $j$ and weighting this similarity by how 'close' region $i$ is to region $j$. Closer regions are given greater weight.

High similarities with high weight (similar values close together) yield high values of spatial autocorrelation. Low similarities with high weight (dissimilar values close together) yield low values of spatial autocorrelation.

Let $\hbox{sim}_{ij}$ denote the similarity between values $Y_i$ and $Y_j$, and let $w_{ij}$ denote a set of weights describing the 'distance' between regions $i$ and $j$, for $i$, $j$ = 1, ..., $N$.

A general spatial autocorrelation index (SAI) is given by 
$$
\hbox{SAI} = \frac{\sum_{i,j=1}^N w_{ij}\hbox{sim}_{ij}}{\sum_{i,j=1}^N w_{ij}}
$$ 
which represents the weighted similarity between regions. The set of weights ($w_{ij}$) is called a spatial weights matrix. The spatial weights matrix defines the neighbors for each region and defines the strength of each association.

For cells in a raster under the rook-contiguity criterion, $w_{ij}$ = 1 if cell $i$ and $j$ share a boundary, and 0 if they don't share a boundary. In this case $w_{ij}$ = $w_{ji}$. Also, a cell is not a neighbor of itself so $w_{ii}$ = 0.

Alternatively you can define center locations from a set of polygon regions and let $w_{ij}$ = 1 if the center of region $i$ is near the center of region $j$ and 0 otherwise. In this case you need to decide on the number of nearest neighbors.

You can also define neighbors by distance. For example, if $d_{ij}$ is the distance between centers $i$ and $j$, you can let $w_{ij}$ = 1 if $d_{ij}$ \< $\delta$ and 0 otherwise.

Consider crime data at the tract level in the city of Columbus, Ohio. The tract polygons are planar projected with arbitrary spatial coordinates.

```{r}
if(!"columbus" %in% list.files("data")) {
download.file(url = "http://myweb.fsu.edu/jelsner/temp/data/columbus.zip",
              destfile = here::here("data", "columbus.zip"))
unzip(here::here("data", "columbus.zip"),
      exdir = here::here("data"))
}

( CC.sf <- sf::st_read(dsn = here::here("data", "columbus"),
                       layer = "columbus") )
```

The simple feature data frame contains housing values (`HOVAL`), income values (`INC`) and (`CRIME`) in census tracts across the city. Crime (`CRIME`) is residential burglaries and vehicle thefts per 1000 households. Income (`INC`) and housing values (`HOVAL`) are annual values with units of 1000 dollars.

Create a choropleth map of the crime rates (`CRIME`).

```{r}
tmap::tm_shape(CC.sf) +
  tmap::tm_fill(col = "CRIME",
                title = "Burglary & Vehicle Thefts\n/1000 Households")
```

Note that the variable name CRIME must be in quotes.

Alternatively you create a choropleth map of the crime rates using `geom_sf()`. Here the variable name CRIME is without quotes.

```{r}
library(ggplot2)

ggplot(data = CC.sf) + 
  geom_sf(mapping = aes(fill = CRIME)) +
  labs(fill = "Burglary & Vehicle Thefts\n/1000 Households") +
  theme_void()
```

High (and low) crime areas tend to be clustered.

Autocorrelation quantifies the amount of clustering. To compute the autocorrelation you first need to define the neighbors for each polygon.

You create a list of neighbors using the `spdep::poly2nb()` function from the {spdep} package. Functions in the {spdep} package support S3 and S4 spatial data objects.

The 'nb' in the function names stands for neighbor (list) object. The function builds the list from geometries based on contiguity. Neighbors must share a border. By default contiguity is defined as having at least one location along a border common. This is changed by using the argument `queen = FALSE`.

```{r}
if(!require(spdep)) install.packages("spdep", repos = "http://cran.us.r-project.org")

( nbs <- spdep::poly2nb(CC.sf) )
```

The output tells you there are 49 tracts (polygons). Each tract is bordered by at least one other tract. The average number of neighbors is 4.8. The total number of neighbors over all tracts is 236. This represents 9.8% of all possible connections (if every tract is a neighbor of itself and every other tract 49 \* 49).

Note that this only works for spatial data frames. It will not work for a regular data frame where the spatial coordinates are defined as variables.

A graph of the tracts as nodes and neighbors as links is obtained with the `plot()` method. The arguments include the neighbor list object (here you assigned it the name `nbs`) and the location of the polygon centers, which are extracted from the simple feature data frame using the `sf::st_centroid()`.

```{r}
plot(CC.sf$geometry)
plot(nbs, 
     sf::st_centroid(CC.sf$geometry),
     add = TRUE)
```

The graph is a network showing the contiguity pattern (adjacency or neighbor structure). Tracts close to the center of the city have more neighboring tracts and thus more links in the network.

The number of links per tract (node)--link distribution--is obtained with the `summary()` method.

```{r}
nbs |>
  summary()
```

The list of neighboring tracts for the first two tracts.

```{r}
nbs[[1]]
nbs[[2]]
```

The first tract has two neighbors that include tracts 2 and 3. The neighbor numbers are stored as an integer vector within the `nb` object. Tract 2 has three neighbors that include tracts 1, 3, and 4. Tract 5 has 8 neighbors and so on. The function `spdep::card()` tallies the number of neighbors by tract.

```{r}
spdep::card(nbs)
```

Tract 5 has 8 neighbors and so on.

The next step is to include weights to the neighbor list object indicating how 'close' each neighbor is. The function `spdep::nb2listw()` turns the neighbor list object into a spatial weights object. By default the weighting scheme gives each link the same weight equal to the multiplicative inverse of the number of neighbors.

```{r}
wts <- nbs |>
  spdep::nb2listw()

class(wts)
```

This `wts` object is a list with two elements. The first element (`listw`) is the weights matrix and the second element (`nb`) is the neighbor list object.

```{r}
wts |>
  summary()
```

The network statistics are given along with information about the weights. The default weighting scheme assigns a weight to each neighbor equal to the inverse of the number of neighbors (`style = "W"`). For a tract with 5 neighbors each neighbor gets a weight of 1/5. The sum over all weights (`S0`) is the number of tracts.

To see the weights for the first two tracts type

```{r}
wts$weights[1:2]
```

The object `weights` represents the weights matrix as a list. The full matrix has dimensions 49 x 49 but most of the entries are zero since for a given tract most tracts are not neighbors.

```{r}
nbs |>
  spdep::nb2mat() |>
  head()
```

To see the neighbors of the first two tracts type

```{r}
wts$neighbours[1:2]
```

Tract 1 has two neighbors (tract 2 & 3) so each are given a weight of 1/2. Tract 2 has three neighbors (tract 1, 3, & 4) so each are given a weight of 1/3.

With the weights matrix saved as an object you are ready to compute a metric of spatial autocorrelation.

Caution: Neighbors defined by contiguity can leave some areas without any neighbors. Islands for example. By default the `spdep::nb2listw()` function assumes each area has at least one neighbor. If this is not the case you need to specify how areas without neighbors are handled using the argument `zero.policy = TRUE`. This permits the weights list to be formed with zero-length weight vectors.

For example, consider the districts in the country of Scotland.

```{r}
if(!"scotlip" %in% list.files(here::here("data"))) {
download.file("http://myweb.fsu.edu/jelsner/temp/data/scotlip.zip",
              destfile = here::here("data", "scotlip.zip"))
unzip(here::here("data", "scotlip.zip"),
      exdir = here::here("data"))
}

SL.sf <- sf::st_read(dsn = here::here("data", "scotlip"), 
                     layer = "scotlip")
plot(SL.sf$geometry)
```

Three of the districts are islands. These districts have no bordering districts.

Create a list of neighbors based on contiguity (default).

```{r}
( nbs2 <- SL.sf |>
    spdep::poly2nb() )
```

Three regions have no neighbors and thus no links.

Use the `spdep::nb2listw()` function with the argument `zero.policy = TRUE`. Otherwise you get an error saying the empty neighbor sets are found.

```{r}
wts2 <- nbs2 |>
  spdep::nb2listw(zero.policy = TRUE)
head(wts2$weights)
```

The sixth region has no neighbors and so the 6th element of the list is set to `NULL`.

Once you have a weights matrix defining neighbors and the associated strength of the neighbor relationship, you are ready to compute various statistics of spatial autocorrelation.

## Computing autocorrelation {.unnumbered}

A commonly used spatial autocorrelation statistic is Moran's I. Moran's I follows the basic form of autocorrelation indexes where the similarity between regions $i$ and $j$ is proportional to the product of the deviations from the mean

$$
\hbox{sim}_{ij} \propto (Y_i - \bar Y) (Y_j - \bar Y)
$$

where $i$ indexes the region and $j$ indexes the neighbors of $i$. The value of $\hbox{sim}_{ij}$ is large when the $Y$ values in the product are on the same side of their respective means (both above or below) and small when they are on opposite sides of their respective means (one above and one below or vice versa).

The formula for I is

$$
\hbox{I} = \frac{N} {W} \frac {\sum_{i,j} w_{ij}(Y_i-\bar Y) (Y_j-\bar Y)} {\sum_{i} (Y_i-\bar Y)^2}
$$ 
where $N$ is the number regions, $w_{ij}$ is the matrix of weights, and $W$ is the sum over all weights.

Consider the following grid of cells containing attribute values.

```{r}
if(!require(spatstat)) install.packages(pkgs = "spatstat", repos = "http://cran.us.r-project.org")
suppressMessages(library(spatstat))

set.seed(6750)
Y <- ppp(runif(200, 0, 1), 
         runif(200, 0, 1))
plot(quadratcount(Y), main = "")
```

The formula for Moran's I results in one value representing the magnitude of the autocorrelation (amount of clustering) over the entire area (global Moran's I).

First consider a single cell in the area ($N$ = 1). Start with the middle cell (row 3, column 3). Let $i$ = 3 in the above formula and let $j$ index the cells touching the center cell in reading order starting with cell (2, 2), then cell (2, 3), etc.

Assume each neighbor is given a weight of 1/8 so $W = \sum_{j=1}^8 w_j = 1$. Then the value of I for the single center cell is I\_{3, 3} = (6 - mean(y)) \* ((8 - mean(y)) + (3 - mean(y)) + (9 - mean(y)) + (12 - mean(y)) + (10 - mean(y)) + (10 - mean(y)) + (9 - mean(y))) / (6 - mean(y))\^2)

```{r}
y <- c(3, 10, 7, 12, 5, 11, 8, 3, 9, 12, 
      6, 12, 6, 10, 3, 8, 10, 10, 9, 7, 
      5, 10, 8, 5, 11)
( yb <- mean(y) )
```

```{r}
Inum_i <- (6 - yb) * 
                 ((8 - yb) + (3 - yb) + (9 - yb) + 
                  (12 - yb) + (10 - yb) + (10 - yb) + 
                  (10 - yb) + (9 - yb))
Iden_i <- (6 - yb)^2
Inum_i/Iden_i
```

The I value of -3.5 indicates that the center cell, which has a value below the average over all 25 cells, is mostly surrounded by cells having values above the average.

Now repeat this calculation for every cell and then take the sum over all the 25 I values.

This is what the function `spdep::moran()` from the {spdep} package does. The first argument is the vector containing the values for which you are interested in determining the magnitude of the spatial autocorrelation and the second argument is the `listw` object.

Further, you need to specify the number of regions and the sum of the weights `S0`. The latter is obtained from the `spdep::Szero()` function applied to the `listw` object.

Returning to the Columbus crime data here let `m` be the number of census tracts and `s` be the sum of the weights. You then apply the `spdep::moran()` function on the variable `CRIME`.

```{r}
( m <- CC.sf$CRIME |>
  length() )
( s <- wts |>
  spdep::Szero() )

CC.sf$CRIME |>
  spdep::moran(listw = wts, 
               n = m, 
               S0 = s)
```

The function returns the value of Moran's I and the kurtosis (K) of the distribution of crime values. Moran's I ranges from -1 to +1.

The value of .5 for the crime rates indicates a high level of spatial autocorrelation. This is expected based on the clustering of crime in the central city.

Positive values of Moran's I indicate clustering and negative values indicate inhibition. Inhibition is a process leading to nearby values having attribute values that are opposite in magnitude to those at each location (like a checkerboard pattern)

Kurtosis is a statistic that indicates how peaked the distribution of the attribute values is. A normal distribution has a kurtosis of 3. If the kurtosis is too large (or small) relative to a normal distribution then any statistical inference we make with Moran's I will be suspect.

Read more about Moran's I here [Wikipedia Moran's I](https://en.wikipedia.org/wiki/Moran%27s_I)

Another statistic that indicates the amount of spatial autocorrelation is Geary's C. The equation is

$$
\hbox{C} = \frac{(N-1) \sum_{i,j} w_{ij} (Y_i-Y_j)^2}{2 W \sum_{i}(Y_i-\bar Y)^2} 
$$ 
where $W$ is the sum over all weights ($w_{ij}$) and $N$ is the number of areas.

The syntax of the `spdep::geary()` function is similar that of `spdep::moran()` except you also need to specify `n1` as one minus the number of areas.

```{r}
CC.sf$CRIME |>
  spdep::geary(listw = wts,
               n = m, 
               S0 = s, 
               n1 = m - 1)
```

Values for Geary's C range from 0 to 2 with 1 indicating no autocorrelation. Values less than 1 indicate positive autocorrelation. Both I and C are global measures of autocorrelation, but C is more sensitive to local variations in autocorrelation.

Rule of thumb: If the interpretation of Geary's C is much different than the interpretation of Moran's I then consider computing local measures of autocorrelation.

More information on Geary's C statistic here [Wikipedia Geary's C](https://en.wikipedia.org/wiki/Geary%27s_C)

## Spatial lag and its relation to autocorrelation {.unnumbered}

Understanding Moran's I is simplified by noted that it is the slope coefficient resulting from a regression of the average neighborhood values onto the observed values. An average of neighborhood values is called the spatial lag variable.

To see this, let `crime` be the set of crime values (from the Columbus data) in each region as a data vector. You create a spatial lag variable using the `spdep::lag.listw()` function. The first argument is the `listw` object and the second is the vector of crime values.

```{r}
crime <- CC.sf$CRIME
Wcrime <- spdep::lag.listw(wts, 
                           crime)
```

For each value in the vector `crime` there is a corresponding value in the vector `Wcrime` representing the average crime over the neighboring regions.

To verify, recall tract 1 had tract 2 and 3 as its two neighbors. So the following code should return a `TRUE`.

```{r}
(crime[2] + crime[3])/2 == Wcrime[1] 
```

The lagged value of crime in tract 1 is equal to the average of the crime in the two neighboring tracts.

Make a scatter plot showing individual observed crime rates on the horizontal axis and the corresponding neighborhood average crime rates on the vertical axis.

```{r}
data.frame(crime, Wcrime) |>
ggplot(mapping = aes(x = crime, y = Wcrime)) + 
  geom_point() + 
  geom_smooth(method = lm) +
  scale_x_continuous(limits = c(0, 70)) +
  scale_y_continuous(limits = c(0, 70)) +
  xlab("Crime") + 
  ylab("Average crime in the neighborhood") +
  theme_minimal()
```

The range of neighborhood average crime rates is smaller than the range of observed crime rates since the mean of a set of observations will always have less variance (dispersion) than the observations.

The scatter plot shows that tracts with low values of crime tend to be surrounded by tracts with low values of crime on average and tracts with high values of crime tend be surrounded by tracts with high values of crime. The slope on the regression line is upward (positive).

The magnitude of the slope is Moran's I. To check this use the `lm()` function from the base set of packages. The function is used to fit linear regression models.

```{r}
lm(Wcrime ~ crime)
```

The coefficient on the `crime` variable in the linear regression is .5.

The scatter plot is called a 'Moran's scatter plot.'

Let's consider another data set.

```{r}
if(!"sids2" %in% list.files(here::here("data"))) {
download.file("http://myweb.fsu.edu/jelsner/temp/data/sids2.zip",
              destfile = here::here("data", "sids2.zip"))
unzip(here::here("data", "sids2.zip"),
      exdir = here::here("data"))
}

SIDS.sf <- sf::st_read(dsn = here::here("data", "sids2")) |>
  sf::st_set_crs(4326)
head(SIDS.sf)
```

The column `SIDR79` contains the death rate (per 1000 live births) (1979-84) from sudden infant death syndrome. Create a choropleth map of the SIDS rates.

```{r}
tmap::tm_shape(SIDS.sf) +
  tmap::tm_fill("SIDR79", title = "") +
  tmap::tm_borders(col = "gray70") +
  tmap::tm_layout(title = "SIDS Rates 1979-84 [per 1000]",
                  legend.outside = TRUE)
```

Create a neighborhood list (`nb`) and a `listw` object (`wts`) then graph the neighborhood network.

```{r}
nbs <- spdep::poly2nb(SIDS.sf)
wts <- spdep::nb2listw(nbs)

plot(nbs, 
     sf::st_centroid(sf::st_geometry(SIDS.sf)))
```

Next compute Moran's I on the SIDS rates using the variable `SIDR79` (1974-1979).

```{r}
m <- length(SIDS.sf$SIDR79)
s <- spdep::Szero(wts)

spdep::moran(SIDS.sf$SIDR79, 
             listw = wts, 
             n = m, 
             S0 = s)
```

I is .14 and K is 4.4. A normal distribution has a kurtosis of 3. Values less than about 2 or greater than about 4 indicate that inferences about autocorrelation based on the assumption of normality are suspect.

Weights are specified using the `style =` argument in the `nb2listw()` function. The default "W" is row standardized (sum of the weights over all links equals the number of polygons). "B" is binary (each neighbor gets a weight of one). "S" is a variance stabilizing scheme.

Each style gives a somewhat different value for I.

```{r}
x <- SIDS.sf$SIDR79
spdep::moran.test(x, spdep::nb2listw(nbs, style = "W"))$estimate[1]
spdep::moran.test(x, spdep::nb2listw(nbs, style = "B"))$estimate[1]  # binary
spdep::moran.test(x, spdep::nb2listw(nbs, style = "S"))$estimate[1]  # variance-stabilizing
```

When reporting a Moran's I you need to state what type of weighting was used.

Let `sids` be a vector with elements containing the SIDS rate in each county. You create a spatial lag variable using the `spdep::lag.listw()` function. The first argument is the `listw` object and the second is the vector of rates.

```{r}
sids <- SIDS.sf$SIDR79
Wsids <- spdep::lag.listw(wts, 
                          sids)
```

For each value in the vector `sids` there is a corresponding value in the object `Wsids` representing the neighborhood average SIDS rate.

```{r}
Wsids[1]
j <- wts$neighbours[[1]]
j
sum(SIDS.sf$SIDR79[j])/length(j)
```

The weight for county one is `Wsids[1]` = 2.659. The neighbor indexes for this county are in the vector `wts$neighbours[[1]]` of length 3. Add the SIDS rates from those counties and divide by the number of counties (`length(j)`).

A scatter plot of the neighborhood average SIDS rate versus the actual SIDS rate in each region.

```{r}
data.frame(sids, Wsids) |>
ggplot(mapping = aes(x = sids, y = Wsids)) + 
  geom_point() + 
  geom_smooth(method = lm) +
  scale_x_continuous(limits = c(0, 7)) +
  scale_y_continuous(limits = c(0, 7)) +
  xlab("SIDS") + ylab("Spatial Lag of SIDS") +
  theme_minimal()
```

The regression line slopes upward indicating positive spatial autocorrelation. The value of the slope is Moran's I. To check this type

```{r}
lm(Wsids ~ sids)
```

## Other neighbor definitions {.unnumbered}

Other neighbor definitions are possible and the choice will influence the value of any statistic of spatial autocorrelation.

Let's consider the historical demographic data in Mississippi counties. Import the data as a simple feature data frame and assign the geometry a geographic CRS.

```{r}
if(!"police" %in% list.files(here::here("data"))) {
download.file(url = "http://myweb.fsu.edu/jelsner/temp/data/police.zip",
              destfile = here::here("data", "police.zip"))
unzip(here::here("data", "police.zip"),
      exdir = here::here("data"))
}

( PE.sf <- sf::st_read(dsn = here::here("data", "police"), 
                 layer = "police") |>
  sf::st_set_crs(4326) )
```

Variables in the simple feature data frame include police expenditures (`POLICE`), crime (`CRIME`), income (`INC`), unemployment (`UNEMP`) and other socio-economic characteristics across Mississippi at the county level. Police expenditures are per person 1982 (dollars per person). Personal income is per person in 1982 (dollars per person). Crime is the number of serious crimes per 100,000 person in 1981. Unemployment is percent of people looking for work in 1980.

The geometries are polygons that define the county borders.

```{r}
library(ggplot2)

ggplot(data = PE.sf) +
  geom_sf()
```

To estimate autocorrelation for any variable in the data frame, you need to first assign the neighbors and weights for each region.

The default options in the `spdep::poly2nb()` and `spdep::nb2listw()` result in neighbors defined by 'queen' contiguity (polygon intersections can include a single point) and weights defined by row standardization (the sum of the weights equals the number of regions).

```{r}
nbs <- spdep::poly2nb(PE.sf)
wts <- spdep::nb2listw(nbs)
```

Alternatively you can specify the number of neighbors and then assign neighbors based on proximity (closeness). Here you first extract the coordinates of the polygon centroids as a matrix.

```{r}
coords <- PE.sf |>
  sf::st_centroid() |>
  sf::st_coordinates()
head(coords)
```

Then use the `spdep::knearneigh()` function on the coordinate matrix and specify the number of neighbors with the `k =` argument. Here you set it to six. That is, allow each county to have 6 closest neighbors.

Since the CRS is geographic you need to include the `longlat = TRUE` argument so distances are calculated using great circles.

```{r}
knn <- spdep::knearneigh(coords, 
                         k = 6, 
                         longlat = TRUE)
names(knn)
head(knn$nn)
```

The output is a list of five elements with the first element a matrix with the row dimension the number of counties and the column dimension the number of neighbors.

Note that by using distance to define neighbors the matrix is not symmetric. For example, county 3 is a neighbor of county 2, but county 2 is not a neighbor of county 3.

Certain spatial regression models require the neighbor matrix to be symmetric. That is if region X is a neighbor of region Y then region Y must be a neighbor of region X.

You turn this matrix into a neighbor object (class `nb`) with the `spdep::knn2nb()` function.

```{r}
nbs2 <- spdep::knn2nb(knn)
summary(nbs2)
```

If you include the argument `sym = TRUE` in the `knn2nb()` function then it forces the neighbor matrix to be symmetric.

```{r}
nbs3 <- spdep::knn2nb(knn,
                      sym = TRUE)
summary(nbs3)
```

The result shows that six is now the minimum number of nearest neighbors with some counties having has many as 10 neighbors to guarantee symmetry.

Compare the default adjacency neighborhoods with the nearest-neighbor neighborhoods.

```{r}
plot(sf::st_geometry(PE.sf), border = "grey")
plot(nbs, coords, add = TRUE)

plot(sf::st_geometry(PE.sf), border = "grey")
plot(nbs2, coords, add = TRUE)
```

A difference between the two neighborhoods is the number of links on counties along the borders. The nearest-neighbor defined neighborhoods have more links. Note: when neighbors are defined by proximity counties can share a border but they still may not be neighbors.

Your choice of neighbors is based on domain specific knowledge. If the process you are interested in can be described by a dispersal mechanism then proximity definition might be the right choice for defining neighbors. If the process can be described by a border diffusion mechanism then contiguity might be the right choice.

Create weight matrices for these alternative neighborhoods using the same `spdep::nb2listw()` function.

```{r}
wts2 <- spdep::nb2listw(nbs2)
wts3 <- spdep::nb2listw(nbs3)
```

You compute Moran's I for the percentage of white people variable (`WHITE`) with the `moran()` function separately for the three different weight matrices.

```{r}
spdep::moran(PE.sf$WHITE,
             listw = wts,
             n = length(nbs),
             S0 = spdep::Szero(wts))

spdep::moran(PE.sf$WHITE,
             listw = wts2,
             n = length(nbs2),
             S0 = spdep::Szero(wts2))

spdep::moran(PE.sf$WHITE,
             listw = wts3,
             n = length(nbs3),
             S0 = spdep::Szero(wts3))
```

Values of Moran's I are constrained between -1 and +1. In this case the neighborhood definition has little or no impact on inferences made about spatial autocorrelation. The kurtosis is between 2 and 4 consistent with a set of values from a normal distribution.

In a similar way you compute the Geary's C statistic.

```{r}
spdep::geary(PE.sf$WHITE, 
             listw = wts,
             n = length(nbs), 
             S0 = spdep::Szero(wts), 
             n1 = length(nbs) - 1)
```

Values of Geary's C range between 0 and 2 with values less than one indicating positive autocorrelation.

If the values of Moran's I and Geary's C result in different interpretations about the amount of clustering then it is a good idea to examine *local* variations in autocorrelation.
