# Thursday February 16, 2023 {-}

**"Be curious. Read widely. Try new things. I think a lot of what people call intelligence boils down to curiosity."** - Aaron Swartz

On Tuesday
Defining spatial neighborhoods and spatial weights
Computing autocorrelation
Relating autocorrelation to the spatial lag variable

## Defining neighborhoods {-}

Other neighbor definitions are possible and the neighborhood choice will influence the value of any spatial autocorrelation statistic

Let's consider the historical demographic data in Mississippi counties. Import the data as a simple feature data frame and assign the geometry a geographic CRS

```{r}
if(!"police" %in% list.files(here::here("data"))) {
download.file(url = "http://myweb.fsu.edu/jelsner/temp/data/police.zip",
              destfile = here::here("data", "police.zip"))
unzip(here::here("data", "police.zip"),
      exdir = here::here("data"))
}

( PE.sf <- sf::st_read(dsn = here::here("data", "police"), 
                 layer = "police") |>
  sf::st_set_crs(4326) )
```

Variables in the simple feature data frame include police expenditures (`POLICE`), crime (`CRIME`), income (`INC`), unemployment (`UNEMP`) and other socio-economic characteristics across Mississippi at the county level. Police expenditures are per person 1982 (dollars per person). Personal income is per person in 1982 (dollars per person). Crime is the number of serious crimes per 100,000 person in 1981. Unemployment is percent of people looking for work in 1980

Geometries are polygons that define the county borders

```{r}
library(ggplot2)

ggplot(data = PE.sf) +
  geom_sf()
```

To estimate autocorrelation for any variable in the data frame, you need to first assign the neighbors and weights for each region

The default options in the `spdep::poly2nb()` and `spdep::nb2listw()` result in neighbors defined by 'queen' contiguity (polygon intersections can include a single point) and weights defined by row standardization (the sum of the weights equals the number of regions)

```{r}
nbs <- PE.sf |>
  spdep::poly2nb()
wts <- nbs |>
  spdep::nb2listw()
```

Alternatively you can specify the number of neighbors and then assign neighbors based on proximity (closeness). Here you first extract the coordinates of the polygon centroids as a matrix

```{r}
coords <- PE.sf |>
  sf::st_centroid() |>
  sf::st_coordinates()
head(coords)
```

Then use the `spdep::knearneigh()` function on the coordinate matrix and specify the number of neighbors with the `k =` argument. Here you set it to six. That is, allow each county to have 6 closest neighbors

Since the CRS is geographic you need to include the `longlat = TRUE` argument so distances are calculated using great circles

```{r}
knn <- spdep::knearneigh(coords, 
                         k = 6, 
                         longlat = TRUE)
names(knn)
head(knn$nn)
```

The output is a list of five elements with the first element a matrix with the row dimension the number of counties and the column dimension the number of neighbors

Note that by using distance to define neighbors the matrix is not symmetric. For example, county 3 is a neighbor of county 2, but county 2 is not a neighbor of county 3

Certain spatial regression models require the neighbor matrix to be symmetric. That is if region X is a neighbor of region Y then region Y must be a neighbor of region X

You turn this matrix into a neighbor object (class `nb`) with the `spdep::knn2nb()` function

```{r}
nbs2 <- knn |>
  spdep::knn2nb()

summary(nbs2)
```

If you include the argument `sym = TRUE` in the `knn2nb()` function then it forces the neighbor matrix to be symmetric

```{r}
nbs3 <- spdep::knn2nb(knn,
                      sym = TRUE)
summary(nbs3)
```

The result shows that six is now the minimum number of nearest neighbors with some counties having has many as 10 neighbors to guarantee symmetry

Compare the default adjacency neighborhoods with the nearest-neighbor neighborhoods

```{r}
plot(sf::st_geometry(PE.sf), border = "grey")
plot(nbs, coords, add = TRUE)

plot(sf::st_geometry(PE.sf), border = "grey")
plot(nbs2, coords, add = TRUE)
```

A difference is the number of links on counties along the borders. The nearest-neighbor defined neighborhoods have more links. Note: when neighbors are defined by proximity counties can share a border but they still may not be neighbors

Your choice of neighbors should be informed by domain specific knowledge. If the process you are interested in can be described by a dispersal mechanism then proximity definition might be the right choice for defining neighbors. If the process can be described by a border diffusion mechanism then contiguity might be the right choice

Create weight matrices for these alternative neighborhoods using the same `spdep::nb2listw()` function

```{r}
wts2 <- nbs2 |>
  spdep::nb2listw()
wts3 <- nbs3 |>
  spdep::nb2listw()
```

You compute Moran's I for the percentage of white people variable (`WHITE`) with the `moran()` function separately for the three different weight matrices

```{r}
spdep::moran(PE.sf$WHITE,
             listw = wts,
             n = length(nbs),
             S0 = spdep::Szero(wts))

spdep::moran(PE.sf$WHITE,
             listw = wts2,
             n = length(nbs2),
             S0 = spdep::Szero(wts2))

spdep::moran(PE.sf$WHITE,
             listw = wts3,
             n = length(nbs3),
             S0 = spdep::Szero(wts3))
```

Values of Moran's I are constrained between -1 and +1. In this case the neighborhood definition has little or no impact on inferences made about spatial autocorrelation. The kurtosis is between 2 and 4 consistent with a set of values from a normal distribution

In a similar way you compute the Geary's C statistic

```{r}
spdep::geary(PE.sf$WHITE, 
             listw = wts,
             n = length(nbs), 
             S0 = spdep::Szero(wts), 
             n1 = length(nbs) - 1)
```

Values of Geary's C range between 0 and 2 with values less than one indicating positive autocorrelation

If the values of Moran's I and Geary's C result in different interpretations about the amount of clustering then it is a good idea to examine *local* variations in autocorrelation

Before getting to local variants of Moran's I, let's look at how to understand whether the amount of cluster is statistically significant

## Assessing the statistical significance of autocorrelation {-}

Moran's I and Geary's C measure the amount of autocorrelation in spatial data. But it is important to understand that attribute values placed arbitrarily across a spatial domain will result in some amount of autocorrelation just by chance

Statistical tests provide a way to guard against being fooled by this chance autocorrelation. So you ask, is the value of Moran's I significant with respect to the null hypothesis of no autocorrelation?

One way to answer this question is to draw an uncertainty band on the regression line in a Moran scatter plot. If a horizontal line can be placed entirely within the band then the slope, which is equivalent to Moran's I, is not significant against this null hypothesis of no autocorrelation

Continuing with the demographic data in Mississippi counties, compute a spatially-lagged unemployment variable using the default weighting scheme saved in the object `wts`

```{r}
unemp <- PE.sf$UNEMP
Wunemp <- spdep::lag.listw(wts, 
                           unemp)
```

Then make a Moran's scatter plot

```{r}
data.frame(unemp, Wunemp) |>
ggplot(mapping = aes(x = unemp, y = Wunemp)) + 
  geom_point() + 
  geom_smooth(method = lm) +
  xlab("Unemployment") + 
  ylab("Average neighborhood unemployment") +
  theme_minimal()
```

Since a horizontal line can not be placed entirely within the gray band, the slope (Moran's I) is significant against the null hypothesis of no autocorrelation.

More formally the question of whether Moran's I is statistically significant is answered by comparing the standard deviate ($z$ value) of Moran's I to the appropriate value from a standard normal distribution

This is done using the `spdep::moran.test()` function, where the $z$ value is the difference between I and the expected value of I divided by the square root of the variance of I

The function takes a variable name or numeric vector and a spatial weights list object in that order. The argument `randomisation = FALSE` means the variance of I is computed under the assumption of normally distributed unemployment (`UNEMP`) rates

```{r}
( mt <- spdep::moran.test(PE.sf$UNEMP, 
                          listw = wts,
                          randomisation = FALSE) )
```

Moran's I is .218 with a variance of .0045. The $z$ value for I is 3.41 giving a $p$-value of .0003 under the null hypothesis of no autocorrelation. Thus you reject the null hypothesis and conclude there is weak but statistically significant autocorrelation in unemployment rates across Mississippi at the county level

Outputs from the `spdep::moran.test()` function are in the form of a list.

```{r}
str(mt)
```

The list element called `estimate` is a vector of length three containing Moran's I, the expected value of Moran's I under the assumption of no autocorrelation, and the variance of Moran's I

The $z$ value is the difference between I and it's expected value divided by the square root of the variance

```{r}
( mt$estimate[1] - mt$estimate[2] ) / sqrt(mt$estimate[3])
```

The $p$-value is the area under a standard normal distribution curve to the right (`lower.tail = FALSE`) of 3.4102 (`mt$statistic`), the red vertical line in the plot below.

```{r}
pnorm(mt$statistic, 
      lower.tail = FALSE)

curve(dnorm(x), from = -4, to = 4, lwd = 2)
abline(v = mt$statistic, col = 'red')
```

So about .03% of the area lies to the right of the red vertical line in this plot

The $p$-value summarizes the evidence in support of the null hypothesis. The smaller the $p$-value, the less evidence there is in support of the null hypothesis

The small $p$-value tells you that the spatial arrangement of the data is unusual with respect to the null hypothesis

The interpretation of the $p$-value is stated as evidence AGAINST the null hypothesis. This is because interest lies in the null hypothesis being untenable. A $p$-value less than .01 is said to provide _convincing_ evidence against the null, a $p$-value between .01 and .05 is said to provide _moderate_ evidence against the null, and a $p$-value between .05 and .15 is said to provide _suggestive, but inconclusive_ evidence against the null. A $p$-value greater than .15 is said to provide _no_ evidence against the null

Note you do not interpret _no_ evidence as _no_ autocorrelation

Under the assumption of normal distributed and uncorrelated data, the expected value for Moran's I is -1/(n-1) where n is the number of regions 

A check on the distribution of unemployment rates indicates that normality is somewhat suspect. A good way to check the normality assumption is to use the `sm::sm.density()` function from the {sm} package

```{r}
if(!require(sm)) install.packages("sm", repos = "http://cran.us.r-project.org")

sm::sm.density(PE.sf$UNEMP, 
               model = "Normal",
               xlab = "Unemployment Rates")
```

The unemployment rates are less "peaked" (lower kurtosis) than a normal distribution. In this case it is better to use the default `randomisation = TRUE` argument in the `spdep::moran.test()` function

Further, the assumptions underlying Moran's test are sensitive to the form of the graph of neighbor relationships and other factors so results should be checked against a test that involves permutations

A random sampling approach to inference is made with the `spdep::moran.mc()` function. MC stands for Monte Carlo which refers to the city of Monte Carlo in Monaco famous for its gambling casinos

The name of the data vector and the weights list object (`listw`) are required as is the number of permutations (`nsim`). Each permutation is a random rearrangement of the unemployment rates across the counties. This removes the spatial autocorrelation but keeps the non-spatial distribution of the unemployment rates. The neighbor topology and weights remain the same

For each permutation (random shuffle of the data values), I is computed and saved. The $p$-value is obtained as the ratio of the number of permuted I values equal to or exceeding the observed I over the number of permutation plus one. In the case where there are 5 permuted I values greater or equal to the observed value based on 99 simulations, the $p$-value is 5/(99 + 1) = .05

For example, if you want inference on I using 9999 permutations type

```{r}
set.seed(40453)

( mP <- spdep::moran.mc(PE.sf$UNEMP, 
                        listw = wts,
                        nsim = 9999) )
```

Nine of the permutations yield a Moran's I greater than .218, hence the $p$-value as evidence in support of the null hypothesis (the true value for Moran's I is zero) is .0009

Note: you initiate the random number generator with a seed value (any will do) so that the set of random permutations of the values across the domain will be the same each time you run this code chunk. This is important for reproducibility. The default random number generator seed value is determined from the current time (internal clock) and so no random permutations will be identical. To control the seed use the `set.seed()` function

The values of I computed for each permutation are saved in the vector `mP$res`

```{r}
head(mP$res)
tail(mP$res)
```

The last value in the vector is I computed using the data in the correct counties. The $p$-value as evidence in support of the null hypothesis that I is zero is given as

```{r}
sum(mP$res > mP$res[10000])/9999
```

A density graph displays the distribution of permuted I's
```{r}
df <- data.frame(mp = mP$res[-10000])

ggplot(data = df,
       mapping = aes(mp)) + 
  geom_density() + 
  geom_rug() + 
  geom_vline(xintercept = mP$res[10000], 
             color = "red", size = 2) +
  theme_minimal()
```

The density curve is centered just to the left of zero consistent with the theoretical expectation (mean)

What do you do with the knowledge that the unemployment rates have significant autocorrelation? By itself, not much, but it can provide notice that something might be going on in certain regions (hot spot analysis)

The knowledge is more useful after other factors are considered. In the language of statistics, knowledge of significant autocorrelation in the model residuals can help you build a better model

## Computing and interpreting bivariate spatial autocorrelation {-}

The idea of spatial autocorrelation can be extended to two variables. The extension is motivated by the fact that aspatial bi-variate association measures, like Pearson's correlation, do not recognize the spatial arrangement of the regions

Consider the correlation between police expenditure (`POLICE`) and the amount of crime (`CRIME`) in the police expenditure data set

```{r}
police <- PE.sf$POLICE
crime <- PE.sf$CRIME

cor.test(police, crime, 
         method = "pearson")
```

You find a significant (direct) correlation ($p$-value << .01) exists between these two variables

But you also note some significant spatial autocorrelation in each of the variables separately

```{r}
spdep::moran.test(police, 
                  listw = wts)
spdep::moran.test(crime, 
                  listw = wts)
```

The Lee statistic combines the Pearson correlation as an aspatial bi-variate association metric with Moran's I as a uni-variate spatial autocorrelation metric. The formula is

$$
L(x,y) = \frac{n}{\sum_{i=1}^{n}(\sum_{j=1}^{n}w_{ij})^2}
\frac{\sum_{i=1}^{n}(\sum_{j=1}^{n}w_{ij}(x_i-\bar{x})) ((\sum_{j=1}^{n}w_{ij}(y_j-\bar{y}))}{\sqrt{\sum_{i=1}^{n}(x_i - \bar{x})^2} \sqrt{\sum_{i=1}^{n}(y_i - \bar{y})^2}}
$$

The formula is implemented in the `spdep::lee()` function where the first two arguments are the variables of interest and you need to include the weights matrix and the number of regions. The output from this function is a list of two with the first being the value of Lee's statistic (`L`)

```{r}
Lee <- spdep::lee(crime, police, 
                  listw = wts, 
                  n = length(nbs))
Lee$L
```

Values of L range between -1 and +1. The value here of .13 indicates relatively weak bi-variate spatial autocorrelation between crime and police expenditures

You interpret this to mean that crime in a county has a relationship to police expenditure in that county (large Pearson correlation) AND it also has some relationship to police expenditure in the neighboring counties but not much

The `crime` and `police` variables can not be adequately described with a normal distribution.

```{r}
par(mfrow = c(2, 1))
sm::sm.density(crime, model = "normal")
sm::sm.density(police, model = "normal")
```

Thus you perform a non-parametric test on the bi-variate spatial autocorrelation with the `spdep::lee.mc()` function. The crime and police expenditure values are randomly permuted and values of Lee's statistic (`L`) are computed for each permutation

```{r}
spdep::lee.mc(crime, police, 
              listw = wts, 
              nsim = 999)
```

Based on a $p$-value that exceeds .05 you conclude that there is no significant bi-variate spatial autocorrelation between crime and police expenditure in these data

## Assessing local indicators of spatial autocorrelation {-}

The Moran's I statistic was first used in the 1950s. Localization of the statistic was presented by Luc Anselin in 1995 (Anselin, L. 1995. Local indicators of spatial association, Geographical Analysis, 27, 93â€“115)

Earlier you saw the `raster::MoranLocal()` function from the {raster} package returns a raster of local Moran's I values

Local I is a deconstruction of global I where geographic proximity is used in two ways. (1) to define and weight neighbors and (2) to determine the spatial scale over which I is computed

Using queen's contiguity you determine the neighborhood topology and the weights for the police expenditure data from Mississippi. Here you print them in the full matrix form with the `spdep::list2mat()` function

```{r}
round(spdep::listw2mat(wts)[1:5, 1:10], 2)
```

The matrix shows that the first county has three neighbors 2, 3, and 9 and each get a weight of 1/3. The third county has four neighbors 1, 4, 9 and 10 and each gets a weight of 1/4

Compute local Moran's I on the percentage of white people using the `spdep::localmoran()` function. Two arguments are needed (1) the attribute variable for which you want to compute local correlation and (2) the weights matrix as a list object

```{r}
Ii_stats <- spdep::localmoran(PE.sf$WHITE, 
                              listw = wts)
str(Ii_stats)
```

The local I values are stored in the first column of a matrix where the rows are the counties. The other columns are the expected values for I, the variances of I, the $z$ values and the $p$-values. For example, the local I statistics from the first six counties are given by typing

```{r}
head(Ii_stats)
```

Because the local I values must average to the global value (when using row standardized weights), they can take on values outside the range between -1 and 1. A `summary()` method on the first column of the `Li`  object gives statistics from the non-spatial distribution of I's

```{r}
summary(Ii_stats[, 1])
```

To make a map of the values, you start by attaching the matrix columns of interest to the simple feature data frame. Here you attach `Ii` (local Moran's), `Vi` (variance), and `Pi` ($p$-value).

```{r}
PE.sf$Ii <- Ii_stats[, 1]
PE.sf$Vi <- Ii_stats[, 3]
PE.sf$Pi <- Ii_stats[, 5] 
```

Then you use {ggplot2} together with the `geom_sf()` function to make a thematic map.

```{r}
( g1 <- ggplot(data = PE.sf) +
  geom_sf(mapping = aes(fill = Ii)) +
  scale_fill_gradient2(low = "green",
                       high = "blue") )
```

You also map the variances

```{r}
ggplot(data = PE.sf) +
  geom_sf(mapping = aes(fill = Vi)) +
  scale_fill_gradient()
```

Variances are larger for counties near the boundaries because the sample sizes are smaller

Plot the p-values

```{r}
ggplot(data = PE.sf) +
  geom_sf(mapping = aes(fill = Pi)) +
  scale_fill_viridis_c(direction = -1)
```

Compare the map of local autocorrelation with a map of percent white

```{r}
( g2 <- ggplot(data = PE.sf) +
  geom_sf(mapping = aes(fill = WHITE)) +
  scale_fill_gradient(low = "black",
                      high = "white") )
```

Plot them together

```{r}
library(patchwork)

g1 + g2
```

Areas where percent white is high over the northeast are also areas with the largest spatial correlation. Other areas of high spatial correlation include the Mississippi Valley and in the south. Note the county with the most negative spatial correlation is the county in the northwest with a fairly high percentage of whites neighbored by counties with much lower percentages of whites

Plot as a cluster map

```{r}
PE.sf$Quadr <- attr(Ii_stats, "quadr")$mean

ggplot(data = PE.sf) +
  geom_sf(mapping = aes(fill = Quadr)) 
```

Local values of Lee's bi-variate spatial autocorrelation are available from the `spdep::lee()` function

Here you compute the local Lee statistic for each county, attach the values to the simple feature data frame, then make a thematic map with functions from the {tmap} package

```{r}
lee_stat <- spdep::lee(crime, police, 
                       listw = wts, 
                       n = length(nbs))

PE.sf$localL <- lee_stat$localL

tmap::tm_shape(PE.sf) +
  tmap::tm_fill("localL",
                title = "") +
  tmap::tm_borders(col = "gray70") +
  tmap::tm_layout(title = "Local bi-variate spatial autocorrelation",
                  legend.outside = TRUE)
```

Areas in dark green indicate where the correlation between crime and policing is most influenced by neighboring crime and policing

Example: Population and tornado reports

Is the frequency of tornado reports correlated with the number of people in a region? Might this correlation extend to the number of people in neighboring regions?

To answer these questions you quantify the non-spatial correlation and the bi-variate spatial autocorrelation between tornado occurrences and population. To keep this manageable you focus on one state (Iowa)

Start by getting the U.S. Census data with functions from the {tidycensus} package. Downloading U.S. census data using functions from the {tidycensus} package requires you register with the Census Bureau 

You can get an API key from http://api.census.gov/data/key_signup.html. Then use the `tidycensus::census_api_key()` function and put your key in quotes.
```{r, eval=FALSE}
tidycensus::census_api_key("YOUR API KEY GOES HERE")
```

The `get_decennial()` function grants access to the 1990, 2000, and 2010 decennial US Census data and the `get_acs()` function grants access to the 5-year American Community Survey data. For example, here is how you get county-level population for Iowa

```{r}
Counties.sf <- tidycensus::get_acs(geography = "county", 
                                   variables = "B02001_001E", 
                                   state = "IA",
                                   geometry = TRUE)
```

The code returns a simple feature data frame with county borders as multi-polygons. The variable `B02001_001E` is the 2016-2020 population estimate in each county within the state

Next get the tornado data and count the number of tracks by county. A single track can intersect more than one county

```{r}
Torn.sf <- sf::st_read(dsn = here::here("data", "1950-2021-torn-aspath"), 
                       layer = "1950-2021-torn-aspath") |>
  sf::st_transform(crs = sf::st_crs(Counties.sf)) |>
  dplyr::filter(yr >= 2016)

( TorCounts.df <- Torn.sf |>
  sf::st_intersection(Counties.sf) |>
  sf::st_drop_geometry() |>
  dplyr::group_by(GEOID) |>
  dplyr::summarize(nT = dplyr::n()) )
```

Next join the counts to the simple feature data frame by the common column name `GEOID`

```{r}
Counties.sf <- Counties.sf |>
  dplyr::left_join(TorCounts.df,
                   by = "GEOID") |>
  dplyr::mutate(nT = tidyr::replace_na(nT, 0)) |>
  dplyr::mutate(Area = sf::st_area(Counties.sf),
                rate = nT/Area/(2020 - 2016 + 1) * 10^10,
                lpop = log10(estimate))
```

Note that some counties had no tornadoes and the `dplyr::left_join()` returns a value of `NA` for those. You use `dplyr::mutate()` with `tidyr::replace_na()` to turn those counts to a value of 0.

Make a two-panel map displaying the log of the population and the tornado rates.

```{r}
map1 <- tmap::tm_shape(Counties.sf) +
  tmap::tm_borders(col = "gray70") +
  tmap::tm_fill(col = "lpop",
                title = "Log Population",
                palette = "Blues") +
  tmap::tm_layout(legend.outside = "TRUE")

map2 <- tmap::tm_shape(Counties.sf) +
  tmap::tm_borders(col = "gray70") +
  tmap::tm_fill(col = "rate",
                title = "Annual Rate\n[/10,000 sq. km]",
                palette = "Greens") +
  tmap::tm_layout(legend.outside = "TRUE")

tmap::tmap_arrange(map1, map2)
```

There appears some relationship. The non-spatial correlation between the two variables is obtained with the `cor.test()` function

```{r}
lpop <- Counties.sf$lpop
rate <- as.numeric(Counties.sf$rate)

cor.test(lpop, rate)
```

The bi-variate spatial autocorrelation is assessed using the Lee statistic. A formal non-parametric test under the null hypothesis of no bi-variate spatial autocorrelation is done using a Monte Carlo simulation

```{r}
nbs <- spdep::poly2nb(Counties.sf)
wts <- spdep::nb2listw(nbs)

Lee <- spdep::lee(lpop, rate, 
                  listw = wts, 
                  n = length(nbs))
Lee$L

spdep::lee.mc(lpop, rate, listw = wts, nsim = 9999)
```

Finally you map out the local variation in the bi-variate spatial autocorrelation

```{r}
Counties.sf$localL <- Lee$localL

tmap::tm_shape(Counties.sf) +
  tmap::tm_fill("localL",
                title = "Local Bivariate\nSpatial Autocorrelation") +
  tmap::tm_borders(col = "gray70") +
  tmap::tm_layout(legend.outside = TRUE)
```

What might cause this? Cedar County (dark green) lies between the cities of Cedar Rapids (to the northwest), Iowa City (to the west) and the Quad Cities (to the east). Commuters from neighboring counties into cities like Cedar Rapids provide an ad hoc spotter network for all kinds of phenomenon including severe weather and tornadoes

Repeat this analysis for the state of Kansas and compare/contrast the results and interpretation

Also, compare local Lee with local Moran

```{r}
Ii_stats <- spdep::localmoran(rate, 
                              listw = wts)
Counties.sf$localI = Ii_stats[, 1]

tmap::tm_shape(Counties.sf) +
  tmap::tm_borders(col = "gray70") +
  tmap::tm_fill(col = "localI",
                title = "Local Autocorrelation",
                palette = "Purples") +
  tmap::tm_layout(legend.outside = "TRUE")
```

The clustering of tornado occurrences coincides with the largest population corridor in the state